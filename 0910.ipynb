{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0910DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPB8pVzPrqeryCH9N36k6l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JinHeeeKang/Playdata_Python/blob/master/0910.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gG2LqQNsB5s",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "1. 데이터 로딩  \n",
        "2. 하이퍼파라미터 설정하기  \n",
        "3. 신경망 & 출력층 설계 및 구현    \n",
        "    - 2층짜리 신경망\n",
        "4. 미니배치 구성 & 학습     \n",
        "5. 기울기 (gradient) 계산  \n",
        "6. 가중치 업데이트   \n",
        "7. 학습 경과 기록하기 (loss function)\n",
        "    - 학습 결과가 안좋으면 거꾸로 돌아가보기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEGV8F6Ee0fS",
        "colab_type": "text"
      },
      "source": [
        "## 경사 하강법(수치미분)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzsM5w-qeXCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1.데이터 로딩\n",
        "\n",
        "import tensorflow as tf\n",
        "mnist=tf.keras.datasets.mnist\n",
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "x_train,x_test = x_train/225.0,x_test/255.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdMgBvG_eW_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2.필요한 함수 정의 sigmoid,등\n",
        "\n",
        "def softmax(a):\n",
        "    exp_a=np.exp(a)\n",
        "    sum_exp_a=np.sum(exp_a)\n",
        "    y=exp_a/sum_exp_a\n",
        "    return y\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def cross_entropy_error(y,t):\n",
        "    if y.ndim ==1:\n",
        "        t=t.reshape(1,t.size)\n",
        "        y=y.reshape(1,y.size)\n",
        "\n",
        "    batch_size=y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLfNKaJUeW99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3.네트워크 클래스 정의\n",
        "\n",
        "class TwoLayerNet:\n",
        "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):#가중치 초기화\n",
        "        self.params={}\n",
        "        self.params['W1']=weight_init_std*np.random.randn(input_size,hidden_size)\n",
        "        self.params['W2']=weight_init_std*np.random.randn(hidden_size,output_size)\n",
        "\n",
        "        self.params['b1']=np.zeros(hidden_size)\n",
        "        self.params['b2']=np.zeros(output_size)\n",
        "\n",
        "    #순전파\n",
        "    def predict(self,x):\n",
        "        W1,W2=self.params['W1'],self.params['W2']\n",
        "        b1,b2=self.params['b1'],self.params['b2']\n",
        "\n",
        "        a1=np.dot(x,W1)+b1\n",
        "        z1=sigmoid(a1)\n",
        "\n",
        "        a2=np.dot(z1,W2)+b2\n",
        "        y=softmax(a2)\n",
        "\n",
        "        return y\n",
        "\n",
        "    #손실함수 x:입력데이터, t:정답데이터\n",
        "    #x를 넣어 나온값과 t와의 차이\n",
        "    def loss(self,x,t):\n",
        "        y=self.predict(x)#self. :클래스 안에 있는 요함수를 써라\n",
        "        loss=cross_entropy_error(y,t)\n",
        "        return loss\n",
        "\n",
        "    def accuracy(self,x,t):\n",
        "        y=self.predict(x)\n",
        "        y=np.argmax(y,axis=1)\n",
        "        t=np.argmax(t,axis=1)\n",
        "        accuracy =np.sum(y==t)/ float(x.shape[0])\n",
        "        return accuracy\n",
        "    def numerical_gradient(self,x,t):\n",
        "        loss_W = lambda W:self.loss(x,t)\n",
        "        grads={}\n",
        "        grads['W1']=numerical_gradient(loss_W,self.params['W1'])\n",
        "        grads['W2']=numerical_gradient(loss_W,self.params['W2'])\n",
        "        grads['b1']=numerical_gradient(loss_W,self.params['b1'])\n",
        "        grads['b2']=numerical_gradient(loss_W,self.params['b2'])\n",
        "        return grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhacLdBaeW7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#4.학습 과정 수행\n",
        "\n",
        "#4-1하이퍼파라미터 설정\n",
        "iters_num=10000\n",
        "train_size=x_train_flatten.shape[0]#train data의 전체사이즈\n",
        "batch_size=100\n",
        "learning_rate =0.1\n",
        "\n",
        "#네트워크\n",
        "network=TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
        "\n",
        "train_loss_list=[]\n",
        "#미니배치 구성 & 학습\n",
        "for i in range(iters_num):\n",
        "    #4-2배치데이터 획득\n",
        "    #학습할때는 batch_size 만큼씩 뽑아서 사용\n",
        "    batch_mask = np.random.choice(train_size,batch_size)\n",
        "    x_batch = x_train_flatten[batch_mask]\n",
        "    t_batch = y_train[batch_mask]#정답라벨\n",
        "\n",
        "    #기울기 gradient 계산\n",
        "    grad = network.numerical_gradient(x_batch,t_batch) \n",
        "\n",
        "    #4-3가중치 업데이트\n",
        "    for key in ('W1','W2','b1','b2'):\n",
        "        network.params[key] -= learning_rate *grad[key]\n",
        "    \n",
        "    #4-4학습결과 기록하기 loss function\n",
        "    loss =network.loss(x_batch,t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    print(loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOxwCXoheW49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "34d363ac-6827-45f8-c465-4f940848da28"
      },
      "source": [
        "x_batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzAX2nTGeW0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "531e2a2d-3fe6-4edc-8b52-cfef580b1d90"
      },
      "source": [
        "grad['b1'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0002053552927705482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnLqQpizeWzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#weight가 계산이 안되고 있는것 같음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPUD7lP5uZZ-",
        "colab_type": "text"
      },
      "source": [
        " softmax 함수 부분에 있어서, input parameter값인 a는 1차원 배열을 가정하고 있는데, TwoLayerNet 클래스에서 학습은 배치 단위로 한번에 학습해서 a값이 2차원 배열을 가지고 있어서 모든 값들이 0에 가까운 값으로 return 되는 것이 아닐까요..?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrFlARiajIfl",
        "colab_type": "text"
      },
      "source": [
        "## 오차역전파법\n",
        "책 p. 167"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO4aBDLMeWxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#사과,세금\n",
        "#곱셈노드\n",
        "class MulLayer:\n",
        "    def __init__(self):\n",
        "        self.x=None\n",
        "        self.y=None\n",
        "\n",
        "    def forward(self,x,y):\n",
        "        self.x=x\n",
        "        self.y=y\n",
        "\n",
        "        out = x*y\n",
        "        return out\n",
        "    \n",
        "    def backward(self,dout):\n",
        "        dx=dout+self.x\n",
        "        dy=dout+self.y\n",
        "\n",
        "        return dx,dy\n",
        "\n",
        "#덧셈노드\n",
        "class AddLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self,x,y):\n",
        "        out = x+y\n",
        "        return out\n",
        "    \n",
        "    def backward(self,dout):\n",
        "        dx=dout*1\n",
        "        dy=dout*1\n",
        "\n",
        "        return dx,dy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlTCAT5bh-TZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "apple =100\n",
        "apple_num=2\n",
        "tax=1.1\n",
        "\n",
        "#노드 정의\n",
        "mul_apple_layer=MulLayer()\n",
        "mul_tax_layer=MulLayer()\n",
        "\n",
        "#forward\n",
        "apple_price =mul_apple_layer.forward(apple,apple_num)\n",
        "price=mul_tax_layer.forward(apple_price,tax)\n",
        "\n",
        "#backward\n",
        "dprice=1\n",
        "dapple_price,dtax=mul_tax_layer.backward(dprice)\n",
        "dapple,dapple_num=mul_apple_layer.backward(dapple_price)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FszG7rXyh-RO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7ef941bb-16b5-43a3-87f8-94fc35c48285"
      },
      "source": [
        "print(price)#너무 작은 값은 무시해도 됨\n",
        "print(dapple_price,dtax)\n",
        "print(dapple,dapple_num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "220.00000000000003\n",
            "201 2.1\n",
            "301 203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvsHxZN7CpRm",
        "colab_type": "text"
      },
      "source": [
        "p.170~p.175 까지   \n",
        "그림5-5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgBOvTZkh-P4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        \n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-0CwWV1h-Lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]#batch_size 증요ㅡ\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfAasEAWh-KL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "        \n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "        \n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAATCMM-h-GV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bbrceK3h-ET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v74NiopRh-BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFpAbfEHh9_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JO6vTYLh98d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlXIe1qWh97A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r3oN_JOh932",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYNgf_WTh9zT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIZh09aYh9xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV-zR_8dh9uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy8ym0RqeWuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om0InO9ceWqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YA1Fpxj-mtu",
        "colab_type": "text"
      },
      "source": [
        "## 정리 X"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lhAfwYcCjbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet:\n",
        "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):#가중치 초기화\n",
        "        self.params={}\n",
        "        self.params['W1']=weight_init_std*np.random.randn(input_size,hidden_size)\n",
        "        self.params['W2']=weight_init_std*np.random.randn(hidden_size,output_size)\n",
        "\n",
        "        self.params['b1']=np.zeros(hidden_size)\n",
        "        self.params['b2']=np.zeros(output_size)\n",
        "\n",
        "    #순전파\n",
        "    def predict(self,x):\n",
        "        W1,W2=self.params['W1'],self.params['W2']\n",
        "        b1,b2=self.params['b1'],self.params['b2']\n",
        "\n",
        "        a1=np.dot(x,W1)+b1\n",
        "        z1=sigmoid(a1)\n",
        "\n",
        "        a2=np.dot(z1,W2)+b2\n",
        "        y=softmax(a2)\n",
        "\n",
        "        return y\n",
        "\n",
        "    #손실함수 x:입력데이터, t:정답데이터\n",
        "    #x를 넣어 나온값과 t와의 차이\n",
        "    def loss(self,x,t):\n",
        "        y=self.predict(x)#self. :클래스 안에 있는 요함수를 써라\n",
        "        loss=cross_entropy_error(y,t)\n",
        "        return loss\n",
        "\n",
        "    def accuracy(self,x,t):\n",
        "        y=self.predict(x)\n",
        "        y=np.argmax(y,axis=1)\n",
        "        t=np.argmax(t,axis=1)\n",
        "        accuracy =np.sum(y==t)/ float(x.shape[0])\n",
        "        return accuracy\n",
        "    def numerical_gradient(self,x,t):\n",
        "        loss_W = lambda W:self.loss(x,t)\n",
        "        grads={}\n",
        "        grads['W1']=numerical_gradient(loss_W,self.params['W1'])\n",
        "        grads['W2']=numerical_gradient(loss_W,self.params['W2'])\n",
        "        grads['b1']=numerical_gradient(loss_W,self.params['b1'])\n",
        "        grads['b2']=numerical_gradient(loss_W,self.params['b2'])\n",
        "        return grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2D26Ii4IBEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#기울기 계산\n",
        "def numerical_gradient(f,x):\n",
        "    h= 1e-4\n",
        "    grad=np.zeros_like(x)\n",
        "\n",
        "    for idx in range(x.shape[0]):\n",
        "        tmp_val =x[idx]\n",
        "        #y증가량 /x증가량 =기울기\n",
        "        #y증가량 : f(x+h) -f(x-h)\n",
        "        #f(x+h)\n",
        "        x[idx] = tmp_val +h\n",
        "        fxh1 = f(x)\n",
        "\n",
        "        #f(x-h)\n",
        "        x[idx] = tmp_val -h\n",
        "        fxh2 = f(x)\n",
        "        \n",
        "        grad[idx]=(fxh1-fxh2)/(2*h)\n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P_tlPgIXYh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvkd81tPChca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(a):\n",
        "    exp_a=np.exp(a)\n",
        "    sum_exp_a=np.sum(exp_a)\n",
        "    y=exp_a/sum_exp_a\n",
        "    return y\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bGxBfUZChVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy_error(y,t):\n",
        "    if y.ndim ==1:\n",
        "        t=t.reshape(1,t.size)\n",
        "        y=y.reshape(1,y.size)\n",
        "    batch_size=y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBZrBOwxWF1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "mnist=tf.keras.datasets.mnist\n",
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "x_train,x_test = x_train/225.0,x_test/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNSgfX8lrwki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_flatten=x_train.reshape([x_train.shape[0],-1])\n",
        "x_test_flatten=x_train.reshape([x_test.shape[0],-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8v4JDFGsxWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(x_train.shape)\n",
        "# print(x_train_flatten.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38rA7F5Bs4Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#하이퍼파라미터 설정\n",
        "iters_num=10000\n",
        "train_size=x_train_flatten.shape[0]\n",
        "batch_size=100\n",
        "learning_rate =0.1\n",
        "\n",
        "#네트워크\n",
        "network=TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
        "\n",
        "train_loss_list=[]\n",
        "#미니배치 구성 & 학습\n",
        "for i in range(iters_num):\n",
        "    #학습할때는 batch_size 만큼씩 뽑아서 사용\n",
        "    batch_mask = np.random.choice(train_size,batch_size)\n",
        "    x_batch = x_train_flatten[batch_mask]\n",
        "    #정답라벨\n",
        "    t_batch = y_train[batch_mask]\n",
        "\n",
        "    #기울기 gradient 계산\n",
        "    grad = network.numerical_gradient(x_batch,t_batch)\n",
        "\n",
        "    #가중치 업데이트\n",
        "    for key in ('W1','W2','b1','b2'):\n",
        "        network.params[key] -= learning_rate *grad[key]\n",
        "    \n",
        "    #학습결과 기록하기 loss function\n",
        "    loss =network.loss(x_batch,t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    print(loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klmdehBfChf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}